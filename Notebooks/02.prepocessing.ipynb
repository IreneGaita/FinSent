{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "ee8f9b85a0d8567f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:18:24.932710Z",
     "start_time": "2025-04-15T14:18:22.981873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ],
   "id": "ebd177e742b70c39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/irene.gaita/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/irene.gaita/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/irene.gaita/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing Dataset Originale",
   "id": "db6cff88bccfa751"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:18:28.878549Z",
     "start_time": "2025-04-15T14:18:26.712802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Caricare il dataset\n",
    "df = pd.read_csv(\"../sentiment_data/all-dataset.csv\", encoding='latin1', names=['label', 'text'])\n",
    "\n",
    "# Rimuovere le stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Funzione per il preprocessing del testo\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Convertire tutto il testo in minuscolo\n",
    "    text = text.lower()\n",
    "\n",
    "    # Rimuovere i numeri\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Rimuovere la punteggiatura\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenizzare il testo\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Rimuovere le stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Lemmatizzazione\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Restituire il testo preprocessato\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Applicare il preprocessing al dataset\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Aggiungere la lunghezza delle frasi originali e preprocessate\n",
    "df['length_original'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['length_preprocessed'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Soglia per le lunghezze estreme (frasi con meno di o uguali a 2 parole)\n",
    "soglia_lunghezza = 2\n",
    "\n",
    "# Filtrare le frasi con lunghezza inferiore o uguale alla soglia\n",
    "df = df[(df['length_original'] > soglia_lunghezza) & (df['length_preprocessed'] > soglia_lunghezza)]\n",
    "\n",
    "# Rimuovere le colonne 'length_original' e 'length_preprocessed'\n",
    "df = df.drop(columns=['length_original', 'length_preprocessed'])\n",
    "\n",
    "# Visualizzare il numero di righe nel dataframe filtrato\n",
    "numero_righe = df.shape[0]\n",
    "print(f\"Numero di righe nel dataframe filtrato: {numero_righe}\")\n",
    "\n",
    "# Esportare il dataframe filtrato (opzionale, se necessario)\n",
    "df.to_csv(\"../sentiment_data/preprocessed_data.csv\", index=False)\n",
    "\n",
    "# Visualizzare le prime righe del dataframe filtrato\n",
    "print(\"\\nDataframe filtrato (senza frasi <= 2 parole):\")\n",
    "print(df.head())\n"
   ],
   "id": "973ae37487114a57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di righe nel dataframe filtrato: 4816\n",
      "\n",
      "Dataframe filtrato (senza frasi <= 2 parole):\n",
      "      label                                               text  \\\n",
      "0   neutral  According to Gran , the company has no plans t...   \n",
      "1   neutral  Technopolis plans to develop in stages an area...   \n",
      "2  negative  The international electronic industry company ...   \n",
      "3  positive  With the new production plant the company woul...   \n",
      "4  positive  According to the company 's updated strategy f...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  accord gran compani plan move product russia a...  \n",
      "1  technopoli plan develop stage area less squar ...  \n",
      "2  intern electron industri compani elcoteq laid ...  \n",
      "3  new product plant compani would increas capac ...  \n",
      "4  accord compani updat strategi year baswar targ...  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing del Testo per la Sentiment Analysis\n",
    "\n",
    "Il preprocessing del testo è un passaggio fondamentale per migliorare la performance dei modelli di machine learning, in quanto aiuta a ridurre la variabilità e a concentrare il modello sulle caratteristiche più rilevanti del testo.\n",
    "\n",
    "### Passaggi di Preprocessing\n",
    "\n",
    "1. **Caricamento del Dataset**\n",
    "   - Il dataset è stato caricato utilizzando la libreria `pandas`:\n",
    "     Il file CSV contiene due colonne: `label` (indicante il sentiment) e `text` (contenente i testi da analizzare).\n",
    "\n",
    "2. **Rimozione delle Stopwords**\n",
    "   - Le **stopwords** (parole comuni come \"the\", \"and\", \"is\") sono state rimosse dal testo. Queste parole non forniscono informazioni significative per la sentiment analysis.\n",
    "     - Utilizzo della libreria `nltk` per ottenere la lista di stopwords in inglese:\n",
    "       ```python\n",
    "       stop_words = set(stopwords.words('english'))\n",
    "       ```\n",
    "\n",
    "3. **Funzione di Preprocessing**\n",
    "   - È stata definita una funzione di preprocessing per elaborare ogni testo nel dataset:\n",
    "     La funzione esegue le seguenti operazioni:\n",
    "\n",
    "     - **Conversione in minuscolo**: Tutto il testo viene convertito in minuscolo per ridurre la variabilità causata dalla maiuscola/minuscola.\n",
    "     - **Rimozione dei numeri**: I numeri vengono rimossi, poiché non sono considerati rilevanti per la sentiment analysis.\n",
    "       ```python\n",
    "       text = re.sub(r'\\d+', '', text)\n",
    "       ```\n",
    "     - **Rimozione della punteggiatura**: I caratteri di punteggiatura (come punti, virgole, ecc.) vengono eliminati, poiché non aggiungono valore al sentiment.\n",
    "       ```python\n",
    "       text = re.sub(r'[^\\w\\s]', '', text)\n",
    "       ```\n",
    "     - **Tokenizzazione**: Il testo viene suddiviso in parole separate utilizzando la libreria `nltk`.\n",
    "       ```python\n",
    "       words = nltk.word_tokenize(text)\n",
    "       ```\n",
    "     - **Rimozione delle stopwords**: Le parole comuni (stopwords) vengono eliminate.\n",
    "     - **Stamming e Lemmatizzazione**: Ogni parola viene ridotta alla sua forma base (lemma) utilizzando il lemmatizzatore `WordNetLemmatizer` della libreria `nltk`. Questo aiuta a ridurre la variabilità delle parole (es. \"running\" diventa \"run\").\n",
    "     - **Eliminazione delle frasi con una lunghezza inferiore o uguale a 2 parole**: Frasi molto corte, come \"welcome\", \"cloud\", \"thik\", o singole parole, spesso non offrono un contesto sufficiente per un'analisi semantica accurata. Queste frasi non contribuiscono in modo significativo alla comprensione del sentiment e potrebbero introdurre rumore nel modello di machine learning.\n",
    "\n",
    "4. **Applicazione del Preprocessing al Dataset**\n",
    "   - La funzione di preprocessing è stata applicata a ogni riga del dataset nella colonna `text`, creando una nuova colonna `cleaned_text` che contiene il testo preprocessato."
   ],
   "id": "46b5fe7056eafa23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
